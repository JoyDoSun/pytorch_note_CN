{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.Tensor\n",
    "在本章中，将包括以下内容\n",
    "1. Tensor的直观理解\n",
    "2. Tensor的基本操作：\n",
    "    * 生成\n",
    "    * 基本运算\n",
    "    * 与Numpy的转换\n",
    "    * GPU计算\n",
    "3. 利用Tensor完成简单的3层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor直观理解\n",
    "* 直观理解Tensor就类似Numpy中的array，是一个n维数组，专门用于在pytorch中做**数值计算**，不涉及深度学习网络结构、计算图、梯度计算等。\n",
    "* 不同于Num平移中的array，torch.Tensor能够使用GPU进行加速，一般GPU速度是CPU的50倍\n",
    "* 总结：Tensor是一个单纯的可以用GPU加速的科学计算工具,应与后面介绍的Variable区分开"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor基本操作\n",
    "### Tensor生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x.size(): ', torch.Size([5, 3]))\n",
      "('x.type(): ', 'torch.FloatTensor')\n"
     ]
    }
   ],
   "source": [
    "# 生成未初始化矩阵\n",
    "x = torch.Tensor(5, 3)\n",
    "\n",
    "# 生成随机矩阵\n",
    "x = torch.rand(5, 3)\n",
    "\n",
    "# 得到Tensor size\n",
    "print('x.size(): ', x.size())\n",
    "\n",
    "# 指定生成的Tensor的数据类型\n",
    "dtype = torch.FloatTensor # 使用CPU\n",
    "# dtype = torch.cuda.FloatTensor # 使用GPU\n",
    "x = torch.rand(5, 3).type(dtype)\n",
    "print('x.type(): ',x.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor基本运算\n",
    "Tensor的运算有多种语法实现\n",
    "具体计算详见：http://pytorch.org/docs/torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x, y: \n",
      "-0.6022  0.2793  0.3447\n",
      "-0.2408  0.6655  1.4507\n",
      "[torch.FloatTensor of size 2x3]\n",
      " \n",
      "-0.0998 -0.2380  1.1340\n",
      "-0.0897  0.8716 -0.5151\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "\n",
      "x + y:  \n",
      "-0.7020  0.0413  1.4786\n",
      "-0.3304  1.5371  0.9357\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "\n",
      "torch.add(x, y):  \n",
      "-0.7020  0.0413  1.4786\n",
      "-0.3304  1.5371  0.9357\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "\n",
      "torch.add(x, y, result):  \n",
      "-0.7020  0.0413  1.4786\n",
      "-0.3304  1.5371  0.9357\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "\n",
      "y.add_(x) \n",
      "-0.7020  0.0413  1.4786\n",
      "-0.3304  1.5371  0.9357\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 生成Tensor\n",
    "x = torch.randn(2, 3)\n",
    "y = torch.randn(2, 3)\n",
    "print 'x, y:', x, y\n",
    "print\n",
    "\n",
    "# 下面用4种语法进行Tensor加法\n",
    "# 语法1\n",
    "print 'x + y: ', x + y\n",
    "print\n",
    "\n",
    "# 语法2\n",
    "print 'torch.add(x, y): ', torch.add(x, y)\n",
    "print \n",
    "\n",
    "# 语法3\n",
    "result = torch.Tensor(2, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print 'torch.add(x, y, result): ', result\n",
    "print \n",
    "\n",
    "# 语法4：in-place计算，类似'+='运算，此时直接赋值给y。in-place操作都使用`_`作为后缀。例如，x.copy_(y)\n",
    "y.add_(x)\n",
    "print 'y.add_(x)', y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor与Numpy进行转换\n",
    "* torch Tensor可以转换成Numpy arry，反之亦然\n",
    "* Tensor与numpy进行转换后，两者的内存位置共享，也就是说Tensor的值改变了，对应Numpy的值也改变，反之亦然"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(x_numpy): <type 'numpy.ndarray'>\n",
      "y_tensor.type():  torch.DoubleTensor\n"
     ]
    }
   ],
   "source": [
    "# 生成Tensor与numpy arrya\n",
    "x_tensor = torch.randn(2, 3)\n",
    "y_numpy = np.random.randn(2, 3)\n",
    "\n",
    "# Tensor转换为Numpy\n",
    "x_numpy = x_tensor.numpy()\n",
    "print 'type(x_numpy):', type(x_numpy)\n",
    "\n",
    "# Numpy转换为Tensor\n",
    "y_tensor = torch.from_numpy(y_numpy)\n",
    "print 'y_tensor.type(): ', y_tensor.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_tensor after add 1: \n",
      "\n",
      " 7.0147  6.0188  6.8715\n",
      " 8.2571  6.9253  5.0008\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "\n",
      "x_numpy after add 1: \n",
      "[[ 7.01474714  6.01878738  6.87148237]\n",
      " [ 8.25705528  6.92529106  5.00075436]]\n",
      "\n",
      "y_tensor after add 2:  \n",
      " 3.0406  3.8906  3.0224\n",
      " 1.9635  2.9981  0.5178\n",
      "[torch.DoubleTensor of size 2x3]\n",
      "\n",
      "\n",
      "y_numpy after add 2: \n",
      "[[ 21.04056901  21.89062435  21.02235041]\n",
      " [ 19.9634549   20.99806561  18.51780203]]\n",
      "\n",
      "y_numpy after add 1 without in-place op: \n",
      "[[ 22.04056901  22.89062435  22.02235041]\n",
      " [ 20.9634549   21.99806561  19.51780203]]\n",
      "\n",
      "y_tensor after add 1 without in-place op:  \n",
      " 3.0406  3.8906  3.0224\n",
      " 1.9635  2.9981  0.5178\n",
      "[torch.DoubleTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 各自对应的Tensor与Numpy共享内存，一变俱变\n",
    "x_tensor.add_(1)\n",
    "print 'x_tensor after add 1: \\n', x_tensor\n",
    "print\n",
    "print 'x_numpy after add 1: \\n', x_numpy\n",
    "print\n",
    "\n",
    "np.add(y_numpy, 2, out=y_numpy)\n",
    "print 'y_tensor after add 2: ', y_tensor \n",
    "print \n",
    "print 'y_numpy after add 2: \\n', y_numpy\n",
    "print \n",
    "\n",
    "# 如果没有使用in-place运算，则无法共享内存\n",
    "y_numpy = y_numpy + 1\n",
    "print 'y_numpy after add 1 without in-place op: \\n', y_numpy\n",
    "print\n",
    "print 'y_tensor after add 1 without in-place op: ', y_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在GPU上使用Tensor\n",
    "上诉的操作中，Tensor都使用CPU进行计算。下面将会使用GPU进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x + y on GPU:  \n",
      "-1.3042  0.3206  1.8233\n",
      "-0.5712  2.2025  2.3864\n",
      "[torch.cuda.FloatTensor of size 2x3 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = x.cuda()\n",
    "y = y.cuda()\n",
    "print 'x + y on GPU: ', x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用Tensor建立简单的网络\n",
    "* 完全使用Tensor建立一个3层神经网络\n",
    "* 需要自己完成梯度计算和梯度更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 55129.8789333\n",
      "epoch: 10000 loss: 6.49848133915\n",
      "epoch: 20000 loss: 2.71281233412\n",
      "epoch: 30000 loss: 1.45664574848\n",
      "epoch: 40000 loss: 0.869384531962\n",
      "epoch: 50000 loss: 0.538773518936\n",
      "epoch: 60000 loss: 0.353727315913\n",
      "epoch: 70000 loss: 0.247347185427\n",
      "epoch: 80000 loss: 0.178062128522\n",
      "epoch: 90000 loss: 0.132103537549\n",
      "epoch: 100000 loss: 0.0990644682941\n",
      "epoch: 110000 loss: 0.0731622382732\n",
      "epoch: 120000 loss: 0.0548861216202\n",
      "epoch: 130000 loss: 0.0414879448364\n",
      "epoch: 140000 loss: 0.031505113572\n",
      "epoch: 150000 loss: 0.0240738952406\n",
      "epoch: 160000 loss: 0.0184511786115\n",
      "epoch: 170000 loss: 0.0142171950947\n",
      "epoch: 180000 loss: 0.010975251201\n",
      "epoch: 190000 loss: 0.00849652048041\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn import preprocessing\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "# dtype = torch.cuda.FloatTensor\n",
    "\n",
    "# 载入数据，并预处理\n",
    "X, y = load_boston(return_X_y=True)\n",
    "X = preprocessing.scale(X[:100,:])\n",
    "y = preprocessing.scale(y[:100].reshape(-1, 1))\n",
    "\n",
    "# 定义超参数\n",
    "data_size, D_input, D_output, D_hidden = X.shape[0], X.shape[1], 1, 50\n",
    "lr = 1e-5\n",
    "epoch = 200000\n",
    "\n",
    "# 转换为Tensor\n",
    "# X = torch.Tensor(X).type(dtype)\n",
    "# y = torch.Tensor(y).type(dtype)\n",
    "X = torch.from_numpy(X).type(dtype)\n",
    "y = torch.from_numpy(y).type(dtype)\n",
    "\n",
    "# 定义训练参数\n",
    "w1 = torch.randn(D_input, D_hidden).type(dtype)\n",
    "w2 = torch.randn(D_hidden, D_output).type(dtype)\n",
    "\n",
    "# 进行训练\n",
    "for i in range(epoch):\n",
    "    \n",
    "    # 前向传播\n",
    "    h = torch.mm(X, w1)                # 计算隐层\n",
    "    h_relu = h.clamp(min=0)            # relu\n",
    "    y_pred = torch.mm(h_relu, w2)      # 输出层\n",
    "    \n",
    "    # loss计算，使用L2损失函数\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    if i % 10000 == 0:\n",
    "        print('epoch: {} loss: {}'.format(i, loss))\n",
    "    \n",
    "    # 反向传播，计算梯度\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = X.t().mm(grad_h)\n",
    "\n",
    "    # 更新计算的梯度\n",
    "    w1 -= lr * grad_w1\n",
    "    w2 -= lr * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
