{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 自定义Function\n",
    "本次，我们将学习如何自定义一个Function，下面是本次的主要内容\n",
    "1. 对Function的直观理解\n",
    "2. Function与Module的差异与应用场景\n",
    "3. 写一个简单的ReLU Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 对Function的直观理解\n",
    "* Function实际就是对Variable的运算，如加减乘除，relu，pool等\n",
    "* 与普通Python或者numpy的运算不同，Function是针对计算图，需要计算反向传播的梯度。因此他不仅需要进行该运算（forward过程），还需要保留输入（为计算梯度），并支持反向传播计算梯度。如果有做过公开课cs231的作业，记得里面的每个运算都定义了forward，backward，并通过保存cache来进行反向传播。这两者是类似的。\n",
    "* 在之前Variable的学习中，我们知道进行一次运算后，输出的Variable对应的creator就是其运行的计算，如y = relu(x), y.creator，就是relu这个Function\n",
    "* 我们可以对Function进行拓展，使其满足我们自己的需要，而拓展就需要自定义Function的forward运算，已经对应的backward运算，同时在forward中需要通过保存输入值用于backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Function与Module的差异与应用场景\n",
    "Function与Module都可以对pytorch进行自定义拓展，使其满足网络的需求，但这两者还是有十分重要的不同：\n",
    "* Function一般只定义一个操作，因为其无法保存参数，因此适用于激活函数、pooling等操作；Module是保存了参数，因此适合于定义一层，如线性层，卷积层，也适用于定义一个网络\n",
    "* Function需要定义三个方法：\\__init\\__, forward, backward（需要自己写求导公式）；Module：只需定义\\__init__和forward，而backward的计算由自动求导机制构成\n",
    "* 可以不严谨的认为，Module是由一系列Function组成，因此其在forward的过程中，Function和Variable组成了计算图，在backward时，只需调用Function的backward就得到结果，因此Module不需要再定义backward。\n",
    "* Module不仅包括了Function，还包括了对应的参数，以及其他函数与变量，这是Function所不具备的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 一个ReLU Function\n",
    "1. 首先我们定义一个继承Function的ReLU类\n",
    "2. 然后我们来看Variable在进行运算时，其creator是否是对应的Function\n",
    "3. 最后我们为方便使用这个ReLU类，将其wrap成一个函数，方便调用，不必每次显式都创建一个新对象"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 定义一个ReLU类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        # 在forward中，需要定义MyReLU这个运算的forward计算过程\n",
    "        # 同时可以保存任何在后向传播中需要使用的变量值\n",
    "        self.save_for_backward(input_)         # 将输入保存起来，在backward时使用\n",
    "        output = input_.clamp(min=0)               # relu就是截断负数，让所有负数等于0\n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # 根据BP算法的推导（链式法则），dloss / dx = (dloss / doutput) * (doutput / dx)\n",
    "        # dloss / doutput就是输入的参数grad_output、\n",
    "        # 因此只需求relu的导数，在乘以grad_outpu    \n",
    "        input_, = self.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0                # 上诉计算的结果就是左式。\n",
    "                                                 # 即ReLU在反向传播中可以看做一个通道选择函数，所有未达到阈值（激活值<0）的单元的梯度都为0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 验证Variable与Function的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MyReLU object at 0x7fd0b2d08b30>\n",
      "<__main__.MyReLU object at 0x7fd0b2d08b30>\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "input_ = Variable(torch.randn(1))\n",
    "relu = MyReLU()\n",
    "output_ = relu(input_)\n",
    "\n",
    "# 这个relu对象，就是output_.creator，即这个relu对象将output与input连接起来，形成一个计算图\n",
    "print relu\n",
    "print output_.creator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Wrap一个ReLU函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(input_):\n",
    "    # MyReLU()是创建一个MyReLU对象，\n",
    "    # Function类利用了Python __call__操作，使得可以直接使用对象调用__call__制定的方法\n",
    "    # __call__指定的方法是forward，因此下面这句MyReLU（）（input_）相当于\n",
    "    # return MyReLU().forward(input_)\n",
    "    return MyReLU()(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-3.0000\n",
      "-1.5000\n",
      " 0.0000\n",
      " 1.5000\n",
      " 3.0000\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "Variable containing:\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 1.5000\n",
      " 3.0000\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_ = Variable(torch.linspace(-3, 3, steps=5))\n",
    "print input_\n",
    "print relu(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
