{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable和Function\n",
    "在本节中，主要介绍的是pytorch中构建计算图的方式，主要包括以下内容：\n",
    "\n",
    "1. pytorch如何构建计算图（`Variable`与`Function`类）\n",
    "2. `Variable`与`Tensor`的差别\n",
    "3. 动态图机制是如何进行的（`Variable`与`Function`如何建立计算图）\n",
    "## pytorch如何构建计算图（`Variable`与`Function`）\n",
    "* 一般一个神经网络都可以用一个有向无环图图来表示计算过程，在pytorch中也是构建计算图来实现forward计算以及backward梯度计算。\n",
    "* 计算图由节点和边组成。\n",
    "* 计算图中边相当于一种函数变换或者说运算，节点表示参与运算的数据。边上两端的两个节点中，一个为函数的输入数据，一个为函数的输出数据。\n",
    "* 而`Variable`就相当于计算图中的节点。\n",
    "* `Function`就相当于是计算图中的边，它实现了对一个输入`Variable`变换为另一个输出的`Variable`\n",
    "* 因此，`Variable`需要保存forward时计算的激活值。这个值是一个`Tensor`，可以通过`.data`来得到这个Variable所保存的forward时的计算值。\n",
    "* 同时反向传导时，一个`Variable`还需要保存其梯度。该梯度也是一个`Variable`，可以通过`.grad`来得到。\n",
    "\n",
    "## Variable与Tensor差别\n",
    "* `Tensor`只是一个类似`Numpy array`的数据格式，它可以进行多种运行，但无法构建计算图\n",
    "* `Variable`不仅封装了`Tensor`作为对应的激活值，还保存了产生这个`Variable`的`Function`（即计算图中的边），可以通过`.creator`来看是哪个Function输出了这个Variable\n",
    "* 在forward时，`Variable`和`Function`构建一个计算图。只有得到了计算图，构建了`Variable`与`Function`与Variable的输入输出关系，才能在backward时，计算各个节点的梯度。\n",
    "* `Variable`可以进行`Tensor`的大部分计算\n",
    "* 对`Variable`使用`.backward()`方法，可以得到该Variable之前计算图所有Variable的梯度\n",
    "\n",
    "## 动态图机制是如何进行的（Variable和Function的关系）\n",
    "* Variable与Function组成了计算图\n",
    "* Function是在每次对Variable进行运算生成的，表示的是该次运算\n",
    "* 动态图是在每次forward时动态生成的。具体说，假如有Variable x，Function \\*。他们需要进行运算y = x \\* x，则在运算过程时已经得到一个动态图，该动态图输入是x，输出是y，y的`.creator`是\\*\n",
    "* 一次forward过程将有多个Function连接各个Variable，Function输出的Variable将保存该Function的引用（即.creator），从而组成计算图\n",
    "* 在backward时，将利用这些关系得到backward的值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable基本操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable, Function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "-----------------------\n",
      "\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "None\n",
      "None\n",
      "-----------------------\n",
      "Variable containing:\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "<torch.autograd._functions.basic_ops.AddConstant object at 0x7fa14c600960>\n"
     ]
    }
   ],
   "source": [
    "# 生成Variable\n",
    "x = Variable(torch.ones(2, 2), requires_grad=True) # requires_grad表示在backward是否计算其梯度\n",
    "print(x)\n",
    "\n",
    "print('-----------------------')\n",
    "\n",
    "# 查看Variable的属性.data, .grad, .creator\n",
    "print(x.data)    # Variable保存的值\n",
    "print(x.grad)    # 由于目前还未进行.backward()运算，因此不存在梯度\n",
    "print(x.creator) # 对于手动申明的Variable，不存在creator，即在计算图中，该Variable不存在父节点\n",
    "\n",
    "print('-----------------------')\n",
    "\n",
    "# Variable进行运算\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(y.creator) # y是存在x这个父节点，并且通过'+'这个Function进行连接，因此y.creator是该运算\n",
    "\n",
    "# 查看Variable对应的creator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 进行backward\n",
    "\n",
    "z.backward() # 注意这里等价于z.backward(torch.Tensor([1.0]))，参数表示的是后面的输出对Variable z的梯度\n",
    "print(x.grad)\n",
    "print(y.grad) # 此时y.grad为None，因为backward()只求图中叶子的梯度（即无父节点）,如果需要对y求梯度，则可以使用`register_hook`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable的requires_grad参数与volatile参数\n",
    "* 在创建一个Variable是，有两个bool型参数可供选择，一个是requires_grad，一个是Volatile\n",
    "* requires_grad不是十分对该Var进行计算梯度，一般在finetune是可以用来固定某些层的参数，减少计算。只要有一个叶节点是True，其后续的节点都是True\n",
    "* volatile=True，一般用在训练好网络，只进行inference操作时使用，其不建立Variable与Function的关系。只要有一个叶子节点是True，其后节点都是True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 13]) epoch: 0 loss: 15255.53125\n",
      "epoch: 10000 loss: 6.21921396255\n",
      "epoch: 20000 loss: 2.53848791122\n",
      "epoch: 30000 loss: 1.52961266041\n",
      "epoch: 40000 loss: 1.08377480507\n",
      "epoch: 50000 loss: 0.817189872265\n",
      "epoch: 60000 loss: 0.640373170376\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-43f96a7d1a4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_relu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#     print y_pred.size(), y.size()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch: {} loss: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# 使用loss.data[0]，可以输出Tensor的值，而不是Tensor信息\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xav/.local/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mpow\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPowConstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xav/.local/lib/python2.7/site-packages/torch/autograd/_functions/basic_ops.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, constant, tensor_power)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_power\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPowConstant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_power\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_power\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn import preprocessing\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# dtype = torch.FloatTensor\n",
    "dtype = torch.cuda.FloatTensor\n",
    "X, y = load_boston(return_X_y=True)\n",
    "X = preprocessing.scale(X[:100,:])\n",
    "y = preprocessing.scale(y[:100].reshape(-1, 1))\n",
    "\n",
    "data_size, D_input, D_output, D_hidden = X.shape[0], X.shape[1], 1, 50\n",
    "X = Variable(torch.Tensor(X).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.Tensor(y).type(dtype), requires_grad=False)\n",
    "w1 = Variable(torch.randn(D_input, D_hidden).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(D_hidden, D_output).type(dtype), requires_grad=True)\n",
    "\n",
    "lr = 1e-5\n",
    "epoch = 200000\n",
    "for i in range(epoch):\n",
    "    \n",
    "    # forward\n",
    "    h = torch.mm(X, w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = torch.mm(h_relu, w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if i % 10000 == 0:\n",
    "        print('epoch: {} loss: {}'.format(i, loss.data[0]))    # 使用loss.data[0]，可以输出Tensor的值，而不是Tensor信息\n",
    "    \n",
    "    # backward 我们直接使用Variable.backward()，就能根据forward构建的计算图进行反向传播\n",
    "    loss.backward()\n",
    "      \n",
    "\n",
    "    w1.data -= lr * w1.grad.data                       \n",
    "    w2.data -= lr * w2.grad.data\n",
    "\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 可视化计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 2\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"88pt\" height=\"257pt\"\n",
       " viewBox=\"0.00 0.00 88.00 257.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 253)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-253 84,-253 84,4 -4,4\"/>\n",
       "<!-- 140486167200520 -->\n",
       "<g id=\"node1\" class=\"node\"><title>140486167200520</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"67,-21 13,-21 13,-0 67,-0 67,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"40\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\">Log</text>\n",
       "</g>\n",
       "<!-- 140486167200288 -->\n",
       "<g id=\"node2\" class=\"node\"><title>140486167200288</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"79,-78 1,-78 1,-57 79,-57 79,-78\"/>\n",
       "<text text-anchor=\"middle\" x=\"40\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\">MulConstant</text>\n",
       "</g>\n",
       "<!-- 140486167200288&#45;&gt;140486167200520 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>140486167200288&#45;&gt;140486167200520</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M40,-56.9197C40,-49.9083 40,-40.1442 40,-31.4652\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"43.5001,-31.3408 40,-21.3408 36.5001,-31.3409 43.5001,-31.3408\"/>\n",
       "</g>\n",
       "<!-- 140486167200056 -->\n",
       "<g id=\"node3\" class=\"node\"><title>140486167200056</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"80,-135 0,-135 0,-114 80,-114 80,-135\"/>\n",
       "<text text-anchor=\"middle\" x=\"40\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\">PowConstant</text>\n",
       "</g>\n",
       "<!-- 140486167200056&#45;&gt;140486167200288 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>140486167200056&#45;&gt;140486167200288</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M40,-113.92C40,-106.908 40,-97.1442 40,-88.4652\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"43.5001,-88.3408 40,-78.3408 36.5001,-88.3409 43.5001,-88.3408\"/>\n",
       "</g>\n",
       "<!-- 140486167199824 -->\n",
       "<g id=\"node4\" class=\"node\"><title>140486167199824</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"80,-192 0,-192 0,-171 80,-171 80,-192\"/>\n",
       "<text text-anchor=\"middle\" x=\"40\" y=\"-178.4\" font-family=\"Times,serif\" font-size=\"12.00\">AddConstant</text>\n",
       "</g>\n",
       "<!-- 140486167199824&#45;&gt;140486167200056 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>140486167199824&#45;&gt;140486167200056</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M40,-170.92C40,-163.908 40,-154.144 40,-145.465\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"43.5001,-145.341 40,-135.341 36.5001,-145.341 43.5001,-145.341\"/>\n",
       "</g>\n",
       "<!-- 140486167099920 -->\n",
       "<g id=\"node5\" class=\"node\"><title>140486167099920</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"67,-249 13,-249 13,-228 67,-228 67,-249\"/>\n",
       "<text text-anchor=\"middle\" x=\"40\" y=\"-235.4\" font-family=\"Times,serif\" font-size=\"12.00\">(1)</text>\n",
       "</g>\n",
       "<!-- 140486167099920&#45;&gt;140486167199824 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>140486167099920&#45;&gt;140486167199824</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M40,-227.92C40,-220.908 40,-211.144 40,-202.465\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"43.5001,-202.341 40,-192.341 36.5001,-202.341 43.5001,-202.341\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fc5cea5edd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.visualizer import make_dot\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "\n",
    "# 生成一个计算图y = 0.5*(x + 1)^2;  z = ln(y)\n",
    "x = Variable(torch.ones([1]), requires_grad=True)\n",
    "y = 0.5 * (x + 1).pow(2)\n",
    "z = torch.log(y)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "g = make_dot(z)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
