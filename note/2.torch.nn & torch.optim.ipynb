{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.nn以及torch.optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 torch.nn\n",
    "### 直观理解\n",
    "* 在keras、TFLearn等高级的深度学习集成库中，将许多计算进行更加高级的封装\n",
    "* torch.nn包内就是类似这种高级封装，里面包括了卷积，池化，RNN等计算,以及其他如Loss计算，可以把torch.nn包内的各个类想象成神经网络的一层\n",
    "* 创建一个torch.nn中的层后，对其输入Variable，将得到输出的Variable，具体例子见下示例\n",
    "\n",
    "### 作用\n",
    "* 这种方式将网络高度模块化，使得网络的每个运算都像积木一样一层又一层叠加。\n",
    "* 它的作用包括，能够接受一个输入，并得到一个输出(forward操作)。同时，他保存了该层学习的参数，以及其他的一些状态变量\n",
    "\n",
    "### 实现机制\n",
    "* torch.nn包里面的大部分类都是继承了父类torch.nn.Modules，即本质上所有torch.nn中的层（如卷积、池化）都是torch.nn.Modules\n",
    "* torch.nn.Modules可以简单认为是网络一层，包括该层的参数，以及一些操作，如forward，调用参数等\n",
    "* 同样地，它的作用包括，能够接受一个输入，并得到一个输出(forward操作)。同时，他保存了该层学习的参数，以及其他的一些状态变量。并实现各类功能的方法，如返回Modules的参数等\n",
    "* 因此可以把torch.nn以及torch.nn.Modules看做是一个定义好的小网络（包括其参数），给该网络一个输入，其将得到一个输出。输入输出都是Variable\n",
    "* 更多Modules具体机制以及如何重写将在后面介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型：  Sequential (\n",
      "  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (2): ReLU ()\n",
      "  (3): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      ")\n",
      "模型参数:  <bound method Sequential.parameters of Sequential (\n",
      "  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (2): ReLU ()\n",
      "  (3): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      ")>\n",
      "卷积层参数个数：  torch.Size([16, 1, 5, 5])\n",
      "torch.Size([1, 16, 5, 5])\n",
      "旧的参数值：  \n",
      "(0 ,.,.) = \n",
      "  0.0818  0.1192  0.1884  0.0289  0.1282\n",
      " -0.0269 -0.1857 -0.1634 -0.0330 -0.1028\n",
      "  0.1419  0.1765 -0.0709  0.0598  0.0435\n",
      " -0.1624 -0.0440 -0.0329  0.0813  0.0940\n",
      "  0.1298  0.1757  0.1506  0.0168 -0.0209\n",
      "[torch.FloatTensor of size 1x5x5]\n",
      "\n",
      "新的参数值：  \n",
      "(0 ,.,.) = \n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "[torch.FloatTensor of size 1x5x5]\n",
      "\n",
      "MSELoss (\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.optim\n",
    "* 可以直接使用optim来针对制定的参数进行优化\n",
    "* 每次需先计算出梯度即backward后，才能进行优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.optim 示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建一个optim，并制定对应的参数\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# forward 然后 backward 得到梯度\n",
    "# y = model(x)\n",
    "# y.backward()\n",
    "\n",
    "# 将对应参数梯度置0\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 更新参数\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0760311111808\n",
      "-1.43488168716\n",
      "-4.80305242538\n",
      "-10.8855791092\n",
      "-19.8142986298\n",
      "-31.4813365936\n",
      "-45.7385292053\n",
      "-62.4501876831\n",
      "-81.5586395264\n",
      "-103.017082214\n",
      "-126.720848083\n",
      "-152.583770752\n",
      "-180.485992432\n",
      "-210.315795898\n",
      "-242.073486328\n",
      "-275.739654541\n",
      "-311.207519531\n",
      "-348.420196533\n",
      "-387.39932251\n",
      "-428.143310547\n",
      "-470.586273193\n",
      "-514.656982422\n",
      "-560.32623291\n",
      "-607.590026855\n",
      "-656.414428711\n",
      "-706.7265625\n",
      "-758.504211426\n",
      "-811.717590332\n",
      "-866.39050293\n",
      "-922.446472168\n",
      "-979.888793945\n",
      "-1038.74121094\n",
      "-1098.98596191\n",
      "-1160.60217285\n",
      "-1223.51220703\n",
      "-1287.70996094\n",
      "-1353.17236328\n",
      "-1419.91833496\n",
      "-1488.00378418\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1549610851a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# Calling the step function on an Optimizer makes an update to its parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/share/Anaconda2/lib/python2.7/site-packages/torch/optim/adam.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Code in file nn/two_layer_net_optim.py\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs, and wrap them in Variables.\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, D_out),\n",
    "        )\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Variables it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(20000):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x).mean()\n",
    "\n",
    "    # Compute and print loss.\n",
    "#     loss = loss_fn(y_pred, y)\n",
    "#     print(t, loss.data[0])\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable weights\n",
    "    # of the model)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "#     loss.backward()\n",
    "    y_pred.backward()\n",
    "    if t % 100 == 0:\n",
    "        print(y_pred.data[0])\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
